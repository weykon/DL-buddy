全连接层（FCN）：全连接层执行的是一个线性变换，数学公式为 y = Wx + b，其中 x 是输入，W 是权重矩阵，b 是偏置向量，y 是输出。

卷积层：卷积层执行的是卷积运算。在二维情况下，对于一个大小为 (H, W) 的输入和一个大小为 (h, w) 的卷积核，卷积运算的结果大小为 (H-h+1, W-w+1)。卷积运算是通过在输入上滑动卷积核，并在每个位置计算卷积核和该位置的输入的点积来完成的。

循环层：循环层用于处理序列数据。在最简单的情况下（即普通的RNN），循环层的输出是通过 h_t = f(W_hh * h_{t-1} + W_xh * x_t + b) 计算的，其中 h_t 是时间步 t 的隐藏状态，h_{t-1} 是时间步 t-1 的隐藏状态，x_t 是时间步 t 的输入，W_hh 和 W_xh 是权重矩阵，b 是偏置，f 是激活函数。

池化层：池化层用于降低数据的空间维度。最常见的池化操作是最大池化和平均池化。最大池化是取窗口内的最大值作为输出，平均池化是取窗口内的平均值作为输出。

归一化层：归一化层用于对输入数据进行标准化处理。例如，批量归一化（Batch Normalization）会计算输入数据的均值和方差，然后用 (x - mean) / sqrt(var + eps) 对数据进行标准化，其中 x 是输入，mean 和 var 是均值和方差，eps 是一个小的常数以防止除以零。

激活层：激活层用于引入非线性。例如，ReLU激活函数的数学公式是 f(x) = max(0, x)，Sigmoid激活函数的数学公式是 f(x) = 1 / (1 + exp(-x))。

Dropout层：Dropout层在训练过程中以一定的概率 p 随机将输入的一部分元素设置为0，以防止过拟合。

嵌入层：嵌入层将类别数据映射到一个高维空间的向量。如果有 N 个类别和 D 维的嵌入，那么嵌入层就是一个 NxD 的矩阵，每个类别对应矩阵的一行。
